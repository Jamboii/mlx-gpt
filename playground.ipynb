{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.core.fast as F\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny shakespeare dataset\n",
    "# !curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o input.txt\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text[:1000] # first 1,000 characters\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = mx.array(tokens[:24+1])\n",
    "x = buf[:-1].reshape(4, 6)\n",
    "y = buf[1:].reshape(4,6)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mx.array([[1,2],[3,4]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_gpt2 import GPT, GPTConfig\n",
    "from mlx.utils import tree_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig()\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = dict(tree_flatten(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "sd_hf = model_hf.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_hf.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_hf.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_gpt2 import *\n",
    "import mlx.optimizers as optim\n",
    "import mlx.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=4, T=32)\n",
    "model = GPT(GPTConfig())\n",
    "value_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "\n",
    "num_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    "print(f\"number of parameters: {num_params}\")\n",
    "# for n, p in dict(tree_flatten(model.trainable_parameters())).items():\n",
    "#     print(f\"name: {n:<40} params: {p.size:11d}\")\n",
    "\n",
    "class MyAdamW(optim.AdamW):\n",
    "    def apply_gradients(self, gradients: dict, parameters: dict):\n",
    "        # this function is called for every optimizer.update()\n",
    "        self.updates = []\n",
    "        return super().apply_gradients(gradients, parameters)\n",
    "\n",
    "    def apply_single(self, gradient: mx.array, parameter: mx.array, state: dict):\n",
    "        # apply_single returns the parameter - update = p_updated\n",
    "        # therefore, parameter - p_updated = update\n",
    "        p_updated = super().apply_single(gradient, parameter, state)\n",
    "        self.updates.append((p_updated, parameter - p_updated))\n",
    "        return p_updated\n",
    "\n",
    "# optimize!\n",
    "x, y = train_loader.next_batch()\n",
    "lr = 3e-4\n",
    "optimizer = MyAdamW(learning_rate=lr)\n",
    "ud = []\n",
    "for i in range(10):\n",
    "    # forward pass + loss + backward pass\n",
    "    loss, grads = value_and_grad_fn(model, x, y)\n",
    "    # optimize step\n",
    "    optimizer.update(model, grads)\n",
    "    mx.eval(model.state, optimizer.state)\n",
    "    # DEBUG: checking parameter updates\n",
    "    ud.append(\n",
    "        [\n",
    "            mx.log((mx.std(update) / mx.std(data))).item()\n",
    "            for (data, update) in optimizer.updates\n",
    "        ]\n",
    "    )\n",
    "    print(f\"step: {i}, loss: {loss.item():.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = grads\n",
    "_, g2 = value_and_grad_fn(model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_map(lambda x, y: x + y * (1 / 5), g1, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, (n, p) in enumerate(tree_flatten(model.trainable_parameters())):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "        legends.append(f\"param {n}\")\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k')  # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
